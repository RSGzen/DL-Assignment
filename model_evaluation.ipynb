{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea8f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import os\n",
    "from itertools import cycle\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history, X_test, y_test, class_names, output_dir='outputs'):\n",
    "    # Create output folder\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Predict class probabilities and labels\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # 1. Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nAccuracy Score: {acc:.4f}\")\n",
    "\n",
    "    # 2. Classification Report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # 3. Confusion Matrix\n",
    "    # Raw Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Normalized Confusion Matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'normalized_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Accuracy & Loss Curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # 5. AUC Score (Macro)\n",
    "    #Convert y_true values to binary one-hot encoded format\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n",
    "\n",
    "    # Calculate the MACRO average of AUC using One-vs-Rest (OVR)\n",
    "    auc_macro = roc_auc_score(y_true_bin, y_pred_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    print(f\"\\nMacro AUC Score (OvR): {auc_macro:.4f}\")\n",
    "\n",
    "    # 6. ROC Curve Plot\n",
    "    # Empty Dictionaries:\n",
    "    # fpr = False Positive Rete\n",
    "    # tpr = True Positive Rate\n",
    "    # roc_auc = AUC Score for each class\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    n_classes = len(class_names)\n",
    "    colors = cycle(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2'])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        # Calculate fpr & tpr for each class\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])\n",
    "\n",
    "        # Calculate AUC for each class\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, color=color,\n",
    "                 label=f\"{class_names[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "\n",
    "    #Labelling\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1.5, label='Random Guess', alpha=0.7)\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Multiclass ROC Curve (OvR)\\nMacro AUC = {auc_macro:.4f}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'roc_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # To write all Accuracy Sore, Classification Report & Macro AUC Score in outputs.txt\n",
    "    with open(os.path.join(output_dir, 'outputs.txt'), 'w') as f:\n",
    "        f.write(f\"Accuracy Score: {acc:.4f}\")\n",
    "        f.write(\"\\n\\nClassification Report:\\n\")\n",
    "        f.write(report)\n",
    "        f.write(f\"\\nMacro AUC Score (OvR): {auc_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load model\n",
    "\n",
    "base_path = os.getcwd()\n",
    "modelBase_path = os.path.join(base_path, \"model_files\")\n",
    "\n",
    "model_name = \"model_1\"\n",
    "\n",
    "modelFile_path = os.path.join(modelBase_path, model_name, model_name+\".pth\")\n",
    "\n",
    "# Load entire model including layer and weights\n",
    "model = torch.load(modelFile_path, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load test dataset\n",
    "\n",
    "testDataset_path = os.path.join(base_path, \"processed_data\", \"test\")\n",
    "\n",
    "transformation = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "test_dataset = ImageFolder(testDataset_path, transform=transformation)\n",
    "\n",
    "test_dataLoader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Type of test data: {type(test_dataLoader)}\")\n",
    "print(f\"Shape of test data: {test_dataLoader.shape}\")\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
