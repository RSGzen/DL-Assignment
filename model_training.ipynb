{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f234f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "268c6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# CUSTOM EFFICIENTNET IMPLEMENTATION\n",
    "# =======================\n",
    "\n",
    "class CustomMBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, kernel_size, stride, se_ratio=0.25):\n",
    "        super(CustomMBConvBlock, self).__init__()\n",
    "        self.stride = stride # store the stride for foward pass\n",
    "        self.expand_ratio = expand_ratio # store the expansion ratio for conditional logic\n",
    "        \n",
    "        mid_channels = int(in_channels * expand_ratio)\n",
    "        \n",
    "        self.expand_conv = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False) if expand_ratio != 1 else None # add 1x1 convolutional layer to increase the number of channel\n",
    "        self.bn0 = nn.BatchNorm2d(mid_channels) if expand_ratio != 1 else None # batch normalization for the expand channel\n",
    "        \n",
    "        self.depthwise_conv = nn.Conv2d(\n",
    "            mid_channels if expand_ratio != 1 else in_channels,\n",
    "            mid_channels if expand_ratio != 1 else in_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=kernel_size // 2, #ensure the output spatial dimension are preserved\n",
    "            groups=mid_channels if expand_ratio != 1 else in_channels,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels if expand_ratio != 1 else in_channels) # batch normalize after the depthwise convolution\n",
    "        \n",
    "        se_channels = max(1, int(in_channels * se_ratio))  # Squeeze-and-Excitation (SE) block\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), \n",
    "            nn.Conv2d(mid_channels if expand_ratio != 1 else in_channels, se_channels, kernel_size=1),# reduce channel to sechannel \n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(se_channels, mid_channels if expand_ratio != 1 else in_channels, kernel_size=1), # expand back to the original number of channel \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.reduce_conv = nn.Conv2d( # reduce the number of channel to outchannel using 1x1 convolution\n",
    "            mid_channels if expand_ratio != 1 else in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels) # batch normalization after the reduction convolution\n",
    "        \n",
    "        self.use_residual = in_channels == out_channels and stride == 1 # residual connection is apply if the input and output channel match and the stride is 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x # store x as identity for the residual connection\n",
    "        \n",
    "        if self.expand_ratio != 1: #apply 1x1 convolution , batch normalization and Silu activation \n",
    "            x = self.expand_conv(x)\n",
    "            x = self.bn0(x)\n",
    "            x = nn.functional.silu(x)\n",
    "        \n",
    "        x = self.depthwise_conv(x) # apply depthwise convolution , batch normalization andSilu activation \n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.silu(x)\n",
    "        \n",
    "        se = self.se(x) # compute attention weight through SE block and scales the channel\n",
    "        x = x * se\n",
    "        \n",
    "        x = self.reduce_conv(x) # apply 1x1 convolution and batch normalization to reduce channel\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        if self.use_residual:\n",
    "            x = x + identity\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a00471d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, config, pretrained=False):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, config['stem_channels'], kernel_size=3, stride=2, padding=1, bias=False), # 3x3 convolution with stride 2 (downsampling spatial dimension by 2)\n",
    "            nn.BatchNorm2d(config['stem_channels']),# normalize the output\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_channels = config['stem_channels']\n",
    "        \n",
    "        for stage in config['stages']: # iterate over stage in config. where each stage define a group of blocks\n",
    "            num_layers = stage['num_layers']\n",
    "            out_channels = stage['out_channels']\n",
    "            expand_ratio = stage['expand_ratio']\n",
    "            kernel_size = stage['kernel_size']\n",
    "            stride = stage['stride']\n",
    "            \n",
    "            for i in range(num_layers):\n",
    "                block_stride = stride if i == 0 else 1\n",
    "                self.blocks.append(\n",
    "                    CustomMBConvBlock(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=block_stride,\n",
    "                        se_ratio=0.25\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "        \n",
    "        self.head = nn.Sequential( #final convolutional layer to prepare features for pooling \n",
    "            nn.Conv2d(in_channels, config['head_channels'], kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(config['head_channels']),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1) # average pooling to reduce spatial dimension to 1x1\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(config['head_channels'], 1000)\n",
    "        )\n",
    "        \n",
    "        if not pretrained:\n",
    "            self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d): #set weight to 1 and bias to 0\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x) \n",
    "        for block in self.blocks:\n",
    "            x = block(x) \n",
    "        x = self.head(x) \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a7f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# MODIFIED EFFICIENTNETFER CLASS\n",
    "# =======================\n",
    "\n",
    "class EfficientNetFER(nn.Module):\n",
    "    def __init__(self, num_classes=7, pretrained=False, device='cpu', custom_fc_dims=[512, 256], \n",
    "                 dropout=0.3, use_post_conv=False, efficientnet_config=None):\n",
    "        super(EfficientNetFER, self).__init__()\n",
    "        self.device = device\n",
    "        self.use_post_conv = use_post_conv\n",
    "        \n",
    "        default_config = {\n",
    "            'stem_channels': 32, # number of output channel for initial convolutional layer\n",
    "            'head_channels': 1280, # number of channel before the final classifier\n",
    "            'stages': [\n",
    "                {'num_layers': 1, 'out_channels': 16, 'expand_ratio': 1, 'kernel_size': 3, 'stride': 1},\n",
    "                {'num_layers': 2, 'out_channels': 24, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 2},\n",
    "                {'num_layers': 2, 'out_channels': 40, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 2},\n",
    "                {'num_layers': 3, 'out_channels': 80, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 2},\n",
    "                {'num_layers': 3, 'out_channels': 112, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 1},\n",
    "                {'num_layers': 4, 'out_channels': 192, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 2},\n",
    "                {'num_layers': 1, 'out_channels': 320, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 1},\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.config = efficientnet_config if efficientnet_config else default_config\n",
    "        \n",
    "        self.efficientnet = CustomEfficientNet(self.config, pretrained=pretrained)\n",
    "        self.efficientnet.to(device)\n",
    "        \n",
    "        for param in self.efficientnet.parameters():\n",
    "            param.requires_grad = False # disable gradient computation for all Efficientnet parameter\n",
    "        \n",
    "        if self.use_post_conv:\n",
    "            self.post_conv = nn.Sequential(# additional convolutional block is added after the efficientnet head\n",
    "                nn.Conv2d(self.config['head_channels'], 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512), \n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d(1) # reduce the spatial dimension to 1x1 (global average pooling)\n",
    "            ).to(device)\n",
    "            fc_input_features = 512\n",
    "        else:\n",
    "            fc_input_features = self.config['head_channels'] # 1280\n",
    "        \n",
    "        layers = [nn.Dropout(dropout)] # dropout to prevent overfitting\n",
    "        in_dim = fc_input_features\n",
    "        for out_dim in custom_fc_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            in_dim = out_dim \n",
    "        layers.append(nn.Linear(in_dim, num_classes)) # set a final linear to map the num class\n",
    "        \n",
    "        self.classifier = nn.Sequential(*layers).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.efficientnet.stem(x)\n",
    "        for block in self.efficientnet.blocks:\n",
    "            x = block(x)# iterate through each block , applying the efficientnet architecture\n",
    "        x = self.efficientnet.head(x)\n",
    "        \n",
    "        if self.use_post_conv:\n",
    "            x = self.post_conv(x)\n",
    "        else:\n",
    "            x = self.efficientnet.avgpool(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def unfreeze_layers(self, num_layers=5):# unfreeze specific layer for fine tuning , allow the parameter to be updated during training\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        layers = list(self.efficientnet.blocks[-num_layers:]) + [self.efficientnet.head] # select  the last numlayers blocks of the efficient and head\n",
    "        if self.use_post_conv:\n",
    "            layers.append(self.post_conv)\n",
    "        for layer in layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, device='cuda', patience=5):\n",
    "    #criterion (loss function)\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train() # put layer to training mode ( dropout/batch-norm)\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in tqdm(train_loader, desc='Training'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels) # compute the loss between prediction and true label \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset) \n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset) \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            val_loss = 0.0\n",
    "            val_corrects = 0\n",
    "            with torch.no_grad():# disable the gradient computation as gradient are not needed during validation\n",
    "                for inputs, labels in tqdm(val_loader, desc='Testing'):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1) # get predicted class indices\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    val_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            val_epoch_loss = val_loss / len(val_loader.dataset) \n",
    "            val_epoch_acc = val_corrects.double() / len(val_loader.dataset) \n",
    "            history['test_loss'].append(val_epoch_loss)\n",
    "            history['test_acc'].append(val_epoch_acc.item())\n",
    "            print(f'Test Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
    "        \n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(val_epoch_loss) # use validation loss to decide whether to reduce learning rate \n",
    "            else:\n",
    "                scheduler.step() # update the learning rate based on the epoch count \n",
    "            \n",
    "            if val_epoch_acc > best_acc:\n",
    "                best_acc = val_epoch_acc\n",
    "                best_epoch = epoch\n",
    "            \n",
    "            if epoch - best_epoch > patience:\n",
    "                print(f'Early stopping at epoch {epoch}') # if the validation accuracy hasnt improved , training stop to prevent overfitting \n",
    "                break\n",
    "    \n",
    "    print(f'Best val Acc: {best_acc:.4f} at epoch {best_epoch}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Ti\n",
      "ModuleList(\n",
      "  (0): CustomMBConvBlock(\n",
      "    (depthwise_conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
      "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(48, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(4, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "    (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(72, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(6, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48, bias=False)\n",
      "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(48, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(6, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(48, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=80, bias=False)\n",
      "    (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(80, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(10, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (5): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)\n",
      "    (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(120, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(10, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (6-7): 2 x CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "    (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(240, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(20, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (8): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False)\n",
      "    (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(320, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(20, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(320, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (9-10): 2 x CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(112, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(448, 448, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=448, bias=False)\n",
      "    (bn1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(448, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(28, 448, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(448, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (11): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "    (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (12-14): 3 x CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "    (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (15): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "    (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              SiLU-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      " AdaptiveAvgPool2d-6             [-1, 32, 1, 1]               0\n",
      "            Conv2d-7              [-1, 8, 1, 1]             264\n",
      "              SiLU-8              [-1, 8, 1, 1]               0\n",
      "            Conv2d-9             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-10             [-1, 32, 1, 1]               0\n",
      "           Conv2d-11         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-12         [-1, 16, 112, 112]              32\n",
      "CustomMBConvBlock-13         [-1, 16, 112, 112]               0\n",
      "           Conv2d-14         [-1, 48, 112, 112]             768\n",
      "      BatchNorm2d-15         [-1, 48, 112, 112]              96\n",
      "           Conv2d-16           [-1, 48, 56, 56]             432\n",
      "      BatchNorm2d-17           [-1, 48, 56, 56]              96\n",
      "AdaptiveAvgPool2d-18             [-1, 48, 1, 1]               0\n",
      "           Conv2d-19              [-1, 4, 1, 1]             196\n",
      "             SiLU-20              [-1, 4, 1, 1]               0\n",
      "           Conv2d-21             [-1, 48, 1, 1]             240\n",
      "          Sigmoid-22             [-1, 48, 1, 1]               0\n",
      "           Conv2d-23           [-1, 24, 56, 56]           1,152\n",
      "      BatchNorm2d-24           [-1, 24, 56, 56]              48\n",
      "CustomMBConvBlock-25           [-1, 24, 56, 56]               0\n",
      "           Conv2d-26           [-1, 72, 56, 56]           1,728\n",
      "      BatchNorm2d-27           [-1, 72, 56, 56]             144\n",
      "           Conv2d-28           [-1, 72, 56, 56]             648\n",
      "      BatchNorm2d-29           [-1, 72, 56, 56]             144\n",
      "AdaptiveAvgPool2d-30             [-1, 72, 1, 1]               0\n",
      "           Conv2d-31              [-1, 6, 1, 1]             438\n",
      "             SiLU-32              [-1, 6, 1, 1]               0\n",
      "           Conv2d-33             [-1, 72, 1, 1]             504\n",
      "          Sigmoid-34             [-1, 72, 1, 1]               0\n",
      "           Conv2d-35           [-1, 24, 56, 56]           1,728\n",
      "      BatchNorm2d-36           [-1, 24, 56, 56]              48\n",
      "CustomMBConvBlock-37           [-1, 24, 56, 56]               0\n",
      "           Conv2d-38           [-1, 48, 56, 56]           1,152\n",
      "      BatchNorm2d-39           [-1, 48, 56, 56]              96\n",
      "           Conv2d-40           [-1, 48, 28, 28]           1,200\n",
      "      BatchNorm2d-41           [-1, 48, 28, 28]              96\n",
      "AdaptiveAvgPool2d-42             [-1, 48, 1, 1]               0\n",
      "           Conv2d-43              [-1, 6, 1, 1]             294\n",
      "             SiLU-44              [-1, 6, 1, 1]               0\n",
      "           Conv2d-45             [-1, 48, 1, 1]             336\n",
      "          Sigmoid-46             [-1, 48, 1, 1]               0\n",
      "           Conv2d-47           [-1, 40, 28, 28]           1,920\n",
      "      BatchNorm2d-48           [-1, 40, 28, 28]              80\n",
      "CustomMBConvBlock-49           [-1, 40, 28, 28]               0\n",
      "           Conv2d-50           [-1, 80, 28, 28]           3,200\n",
      "      BatchNorm2d-51           [-1, 80, 28, 28]             160\n",
      "           Conv2d-52           [-1, 80, 28, 28]           2,000\n",
      "      BatchNorm2d-53           [-1, 80, 28, 28]             160\n",
      "AdaptiveAvgPool2d-54             [-1, 80, 1, 1]               0\n",
      "           Conv2d-55             [-1, 10, 1, 1]             810\n",
      "             SiLU-56             [-1, 10, 1, 1]               0\n",
      "           Conv2d-57             [-1, 80, 1, 1]             880\n",
      "          Sigmoid-58             [-1, 80, 1, 1]               0\n",
      "           Conv2d-59           [-1, 40, 28, 28]           3,200\n",
      "      BatchNorm2d-60           [-1, 40, 28, 28]              80\n",
      "CustomMBConvBlock-61           [-1, 40, 28, 28]               0\n",
      "           Conv2d-62          [-1, 120, 28, 28]           4,800\n",
      "      BatchNorm2d-63          [-1, 120, 28, 28]             240\n",
      "           Conv2d-64          [-1, 120, 14, 14]           1,080\n",
      "      BatchNorm2d-65          [-1, 120, 14, 14]             240\n",
      "AdaptiveAvgPool2d-66            [-1, 120, 1, 1]               0\n",
      "           Conv2d-67             [-1, 10, 1, 1]           1,210\n",
      "             SiLU-68             [-1, 10, 1, 1]               0\n",
      "           Conv2d-69            [-1, 120, 1, 1]           1,320\n",
      "          Sigmoid-70            [-1, 120, 1, 1]               0\n",
      "           Conv2d-71           [-1, 80, 14, 14]           9,600\n",
      "      BatchNorm2d-72           [-1, 80, 14, 14]             160\n",
      "CustomMBConvBlock-73           [-1, 80, 14, 14]               0\n",
      "           Conv2d-74          [-1, 240, 14, 14]          19,200\n",
      "      BatchNorm2d-75          [-1, 240, 14, 14]             480\n",
      "           Conv2d-76          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-77          [-1, 240, 14, 14]             480\n",
      "AdaptiveAvgPool2d-78            [-1, 240, 1, 1]               0\n",
      "           Conv2d-79             [-1, 20, 1, 1]           4,820\n",
      "             SiLU-80             [-1, 20, 1, 1]               0\n",
      "           Conv2d-81            [-1, 240, 1, 1]           5,040\n",
      "          Sigmoid-82            [-1, 240, 1, 1]               0\n",
      "           Conv2d-83           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-84           [-1, 80, 14, 14]             160\n",
      "CustomMBConvBlock-85           [-1, 80, 14, 14]               0\n",
      "           Conv2d-86          [-1, 240, 14, 14]          19,200\n",
      "      BatchNorm2d-87          [-1, 240, 14, 14]             480\n",
      "           Conv2d-88          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-89          [-1, 240, 14, 14]             480\n",
      "AdaptiveAvgPool2d-90            [-1, 240, 1, 1]               0\n",
      "           Conv2d-91             [-1, 20, 1, 1]           4,820\n",
      "             SiLU-92             [-1, 20, 1, 1]               0\n",
      "           Conv2d-93            [-1, 240, 1, 1]           5,040\n",
      "          Sigmoid-94            [-1, 240, 1, 1]               0\n",
      "           Conv2d-95           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-96           [-1, 80, 14, 14]             160\n",
      "CustomMBConvBlock-97           [-1, 80, 14, 14]               0\n",
      "           Conv2d-98          [-1, 320, 14, 14]          25,600\n",
      "      BatchNorm2d-99          [-1, 320, 14, 14]             640\n",
      "          Conv2d-100          [-1, 320, 14, 14]           8,000\n",
      "     BatchNorm2d-101          [-1, 320, 14, 14]             640\n",
      "AdaptiveAvgPool2d-102            [-1, 320, 1, 1]               0\n",
      "          Conv2d-103             [-1, 20, 1, 1]           6,420\n",
      "            SiLU-104             [-1, 20, 1, 1]               0\n",
      "          Conv2d-105            [-1, 320, 1, 1]           6,720\n",
      "         Sigmoid-106            [-1, 320, 1, 1]               0\n",
      "          Conv2d-107          [-1, 112, 14, 14]          35,840\n",
      "     BatchNorm2d-108          [-1, 112, 14, 14]             224\n",
      "CustomMBConvBlock-109          [-1, 112, 14, 14]               0\n",
      "          Conv2d-110          [-1, 448, 14, 14]          50,176\n",
      "     BatchNorm2d-111          [-1, 448, 14, 14]             896\n",
      "          Conv2d-112          [-1, 448, 14, 14]          11,200\n",
      "     BatchNorm2d-113          [-1, 448, 14, 14]             896\n",
      "AdaptiveAvgPool2d-114            [-1, 448, 1, 1]               0\n",
      "          Conv2d-115             [-1, 28, 1, 1]          12,572\n",
      "            SiLU-116             [-1, 28, 1, 1]               0\n",
      "          Conv2d-117            [-1, 448, 1, 1]          12,992\n",
      "         Sigmoid-118            [-1, 448, 1, 1]               0\n",
      "          Conv2d-119          [-1, 112, 14, 14]          50,176\n",
      "     BatchNorm2d-120          [-1, 112, 14, 14]             224\n",
      "CustomMBConvBlock-121          [-1, 112, 14, 14]               0\n",
      "          Conv2d-122          [-1, 448, 14, 14]          50,176\n",
      "     BatchNorm2d-123          [-1, 448, 14, 14]             896\n",
      "          Conv2d-124          [-1, 448, 14, 14]          11,200\n",
      "     BatchNorm2d-125          [-1, 448, 14, 14]             896\n",
      "AdaptiveAvgPool2d-126            [-1, 448, 1, 1]               0\n",
      "          Conv2d-127             [-1, 28, 1, 1]          12,572\n",
      "            SiLU-128             [-1, 28, 1, 1]               0\n",
      "          Conv2d-129            [-1, 448, 1, 1]          12,992\n",
      "         Sigmoid-130            [-1, 448, 1, 1]               0\n",
      "          Conv2d-131          [-1, 112, 14, 14]          50,176\n",
      "     BatchNorm2d-132          [-1, 112, 14, 14]             224\n",
      "CustomMBConvBlock-133          [-1, 112, 14, 14]               0\n",
      "          Conv2d-134          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-135          [-1, 672, 14, 14]           1,344\n",
      "          Conv2d-136            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-137            [-1, 672, 7, 7]           1,344\n",
      "AdaptiveAvgPool2d-138            [-1, 672, 1, 1]               0\n",
      "          Conv2d-139             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-140             [-1, 28, 1, 1]               0\n",
      "          Conv2d-141            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-142            [-1, 672, 1, 1]               0\n",
      "          Conv2d-143            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-144            [-1, 192, 7, 7]             384\n",
      "CustomMBConvBlock-145            [-1, 192, 7, 7]               0\n",
      "          Conv2d-146           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-147           [-1, 1152, 7, 7]           2,304\n",
      "          Conv2d-148           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-149           [-1, 1152, 7, 7]           2,304\n",
      "AdaptiveAvgPool2d-150           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-151             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-152             [-1, 48, 1, 1]               0\n",
      "          Conv2d-153           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-154           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-155            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-156            [-1, 192, 7, 7]             384\n",
      "CustomMBConvBlock-157            [-1, 192, 7, 7]               0\n",
      "          Conv2d-158           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-159           [-1, 1152, 7, 7]           2,304\n",
      "          Conv2d-160           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-161           [-1, 1152, 7, 7]           2,304\n",
      "AdaptiveAvgPool2d-162           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-163             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-164             [-1, 48, 1, 1]               0\n",
      "          Conv2d-165           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-166           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-167            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-168            [-1, 192, 7, 7]             384\n",
      "CustomMBConvBlock-169            [-1, 192, 7, 7]               0\n",
      "          Conv2d-170           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-171           [-1, 1152, 7, 7]           2,304\n",
      "          Conv2d-172           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-173           [-1, 1152, 7, 7]           2,304\n",
      "AdaptiveAvgPool2d-174           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-175             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-176             [-1, 48, 1, 1]               0\n",
      "          Conv2d-177           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-178           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-179            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-180            [-1, 192, 7, 7]             384\n",
      "CustomMBConvBlock-181            [-1, 192, 7, 7]               0\n",
      "          Conv2d-182           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-183           [-1, 1152, 7, 7]           2,304\n",
      "          Conv2d-184           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-185           [-1, 1152, 7, 7]           2,304\n",
      "AdaptiveAvgPool2d-186           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-187             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-188             [-1, 48, 1, 1]               0\n",
      "          Conv2d-189           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-190           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-191            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-192            [-1, 320, 7, 7]             640\n",
      "CustomMBConvBlock-193            [-1, 320, 7, 7]               0\n",
      "          Conv2d-194           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-195           [-1, 1280, 7, 7]           2,560\n",
      "            SiLU-196           [-1, 1280, 7, 7]               0\n",
      "          Conv2d-197            [-1, 512, 7, 7]       5,898,752\n",
      "     BatchNorm2d-198            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-199            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-200            [-1, 512, 1, 1]               0\n",
      "         Dropout-201                  [-1, 512]               0\n",
      "          Linear-202                  [-1, 512]         262,656\n",
      "            ReLU-203                  [-1, 512]               0\n",
      "         Dropout-204                  [-1, 512]               0\n",
      "          Linear-205                  [-1, 256]         131,328\n",
      "            ReLU-206                  [-1, 256]               0\n",
      "         Dropout-207                  [-1, 256]               0\n",
      "          Linear-208                    [-1, 7]           1,799\n",
      "================================================================\n",
      "Total params: 9,961,827\n",
      "Trainable params: 6,295,559\n",
      "Non-trainable params: 3,666,268\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 75.30\n",
      "Params size (MB): 38.00\n",
      "Estimated Total Size (MB): 113.88\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Model Setup\n",
    "\n",
    "batch_size = 128 \n",
    "num_epochs = 50 # number of time entire training dataset is passed through model\n",
    "learning_rate = 0.001\n",
    "num_classes = 7\n",
    "\n",
    "if torch.cuda.is_available():# determine to use GPU or CPU for training \n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "train_dir = os.path.join(base_dir, \"processed_data\", \"train\")\n",
    "val_dir = os.path.join(base_dir, \"processed_data\", \"validation\")\n",
    "\n",
    "transformation = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True), # Convert image to float ranging from 0 - 1\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_dir, transform=transformation)\n",
    "val_dataset = ImageFolder(val_dir, transform=transformation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "custom_config = {\n",
    "    'stem_channels': 32, # number of output channel in the initial layer of the network. Stem is the first convolutional layer that process the input image\n",
    "    'head_channels': 1280, #number of channels in the final feature map \n",
    "    'stages': [\n",
    "        {'num_layers': 1, 'out_channels': 16, 'expand_ratio': 1, 'kernel_size': 3, 'stride': 1},\n",
    "        {'num_layers': 2, 'out_channels': 24, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 2},\n",
    "        {'num_layers': 2, 'out_channels': 40, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 2},\n",
    "        {'num_layers': 3, 'out_channels': 80, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 2},\n",
    "        {'num_layers': 3, 'out_channels': 112, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 1},\n",
    "        {'num_layers': 4, 'out_channels': 192, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 2},\n",
    "        {'num_layers': 1, 'out_channels': 320, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 1},\n",
    "    ]\n",
    "}\n",
    "\n",
    "model = EfficientNetFER(\n",
    "    num_classes=num_classes,\n",
    "    device=device,\n",
    "    pretrained=False,\n",
    "    custom_fc_dims=[512, 256],\n",
    "    dropout=0.4,\n",
    "    use_post_conv=True,\n",
    "    efficientnet_config=custom_config\n",
    ")\n",
    "\n",
    "print(model.efficientnet.blocks)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()# combine log softmax and negative log likelihood loss , suitable for multi class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)#reduce learning rate by a factor of 0.1 if validation loss doesnt improve for 3 epochs. Help model converge better\n",
    "\n",
    "# Print out model summary\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77031586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:52<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8262 Acc: 0.2456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8123 Acc: 0.2471\n",
      "Epoch 2/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:51<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8132 Acc: 0.2509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8115 Acc: 0.2471\n",
      "Epoch 3/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:49<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8107 Acc: 0.2506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 29/29 [00:05<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8049 Acc: 0.2471\n",
      "Epoch 4/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:48<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8088 Acc: 0.2503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 29/29 [00:05<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8075 Acc: 0.2471\n",
      "Epoch 5/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:48<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8066 Acc: 0.2508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 29/29 [00:05<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8032 Acc: 0.2471\n",
      "Epoch 6/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:49<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8035 Acc: 0.2504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 29/29 [00:05<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8019 Acc: 0.2471\n",
      "Epoch 7/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 225/225 [00:48<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8022 Acc: 0.2511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 29/29 [00:05<00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8003 Acc: 0.2471\n",
      "Early stopping at epoch 6\n",
      "Best val Acc: 0.2471 at epoch 0\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Model Training\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                          num_epochs=num_epochs, device=device)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff67d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model_1 state dictionary file existed previously. New model_1's state is not saved.\n",
      "\n",
      "model_1 file existed previously. New model_1 is not saved.\n"
     ]
    }
   ],
   "source": [
    "## Saving trained model\n",
    "\n",
    "base_path = os.getcwd()\n",
    "folder_path = os.path.join(base_path, \"model_files\")\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "model_name = \"model_1\"\n",
    "\n",
    "model_path = os.path.join(folder_path, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "modelStateDictName = model_name + \"_weights.pth\"\n",
    "modelEntireName = model_name + \".pth\"\n",
    "\n",
    "modelStateDict_path = os.path.join(model_path, modelStateDictName)\n",
    "modelEntire_path = os.path.join(model_path, modelEntireName)\n",
    "\n",
    "# Save model's weight only\n",
    "if os.path.isfile(modelStateDict_path):\n",
    "    print(f\"\\n{model_name} state dictionary file existed previously. New {model_name}'s state is not saved.\")\n",
    "else:\n",
    "    torch.save(model.state_dict(), modelStateDict_path)\n",
    "    print(f\"\\n{model_name}'s state file has been successfully saved.\")\n",
    "\n",
    "# Save entire model including layers and weights\n",
    "if os.path.isfile(modelEntire_path):\n",
    "    print(f\"\\n{model_name} file existed previously. New {model_name} is not saved.\")\n",
    "else:\n",
    "    torch.save(model, modelEntire_path)\n",
    "    print(f\"\\n{model_name} has been successfully saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "393036b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model_1's history file existed previously. New model_1's history file is not saved.\n",
      "\n",
      "train_loss\n",
      "1. 1.8266\n",
      "2. 1.8150\n",
      "3. 1.8119\n",
      "4. 1.8089\n",
      "5. 1.8049\n",
      "6. 1.8039\n",
      "7. 1.8006\n",
      "8. 1.7989\n",
      "9. 1.7977\n",
      "10. 1.7967\n",
      "11. 1.7898\n",
      "12. 1.7872\n",
      "13. 1.7859\n",
      "14. 1.7916\n",
      "15. 1.7602\n",
      "16. 1.6887\n",
      "17. 1.5597\n",
      "18. 1.3956\n",
      "19. 1.2365\n",
      "20. 1.0348\n",
      "\n",
      "train_acc\n",
      "1. 0.2435\n",
      "2. 0.2499\n",
      "3. 0.2499\n",
      "4. 0.2508\n",
      "5. 0.2501\n",
      "6. 0.2513\n",
      "7. 0.2511\n",
      "8. 0.2505\n",
      "9. 0.2520\n",
      "10. 0.2537\n",
      "11. 0.2548\n",
      "12. 0.2554\n",
      "13. 0.2560\n",
      "14. 0.2540\n",
      "15. 0.2686\n",
      "16. 0.3057\n",
      "17. 0.3679\n",
      "18. 0.4387\n",
      "19. 0.5053\n",
      "20. 0.6010\n",
      "\n",
      "val_loss\n",
      "1. 1.8085\n",
      "2. 1.8110\n",
      "3. 1.8075\n",
      "4. 1.8050\n",
      "5. 1.8023\n",
      "6. 1.8010\n",
      "7. 1.8053\n",
      "8. 1.8023\n",
      "9. 1.8021\n",
      "10. 1.8024\n",
      "11. 1.8004\n",
      "12. 1.8004\n",
      "13. 1.7998\n",
      "14. 1.7936\n",
      "15. 1.7919\n",
      "16. 1.8262\n",
      "17. 1.9394\n",
      "18. 2.0812\n",
      "19. 2.3317\n",
      "20. 2.3627\n",
      "\n",
      "val_acc\n",
      "1. 0.2474\n",
      "2. 0.2471\n",
      "3. 0.2471\n",
      "4. 0.2469\n",
      "5. 0.2505\n",
      "6. 0.2471\n",
      "7. 0.2522\n",
      "8. 0.2505\n",
      "9. 0.2497\n",
      "10. 0.2499\n",
      "11. 0.2485\n",
      "12. 0.2477\n",
      "13. 0.2485\n",
      "14. 0.2558\n",
      "15. 0.2533\n",
      "16. 0.2458\n",
      "17. 0.2410\n",
      "18. 0.2419\n",
      "19. 0.2134\n",
      "20. 0.2329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Writing model's training history dictionary to file\n",
    "\n",
    "json_path = os.path.join(model_path, model_name+\"_history.json\")\n",
    "\n",
    "if os.path.isfile(json_path):\n",
    "    print(f\"\\n{model_name}'s history file existed previously. New {model_name}'s history file is not saved.\\n\")\n",
    "\n",
    "else:\n",
    "    with open(json_path, 'w') as file:\n",
    "        json.dump(history, file)\n",
    "    \n",
    "    print(f\"{model_name}'s training history has been saved.\\n\")\n",
    "\n",
    "# Checking model's training history\n",
    "\n",
    "\n",
    "for val_type in history:\n",
    "    print(val_type)\n",
    "    dataList = history[val_type]\n",
    "    for i in range(0, len(history[val_type])):\n",
    "        print(f\"{i+1}. {dataList[i]:.4f}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
