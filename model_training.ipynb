{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f234f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268c6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# CUSTOM EFFICIENTNET IMPLEMENTATION\n",
    "# =======================\n",
    "\n",
    "class CustomMBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, kernel_size, stride, se_ratio=0.25):\n",
    "        super(CustomMBConvBlock, self).__init__()\n",
    "        self.stride = stride # store the stride for foward pass\n",
    "        self.expand_ratio = expand_ratio # store the expansion ratio for conditional logic\n",
    "        \n",
    "        mid_channels = int(in_channels * expand_ratio)\n",
    "        \n",
    "        self.expand_conv = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False) if expand_ratio != 1 else None # add 1x1 convolutional layer to increase the number of channel\n",
    "        self.bn0 = nn.BatchNorm2d(mid_channels) if expand_ratio != 1 else None # batch normalization for the expand channel\n",
    "        \n",
    "        self.depthwise_conv = nn.Conv2d(\n",
    "            mid_channels if expand_ratio != 1 else in_channels,\n",
    "            mid_channels if expand_ratio != 1 else in_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=kernel_size // 2, #ensure the output spatial dimension are preserved\n",
    "            groups=mid_channels if expand_ratio != 1 else in_channels,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels if expand_ratio != 1 else in_channels) # batch normalize after the depthwise convolution\n",
    "        \n",
    "        se_channels = max(1, int(in_channels * se_ratio))  # Squeeze-and-Excitation (SE) block\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), \n",
    "            nn.Conv2d(mid_channels if expand_ratio != 1 else in_channels, se_channels, kernel_size=1),# reduce channel to sechannel \n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(se_channels, mid_channels if expand_ratio != 1 else in_channels, kernel_size=1), # expand back to the original number of channel \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.reduce_conv = nn.Conv2d( # reduce the number of channel to outchannel using 1x1 convolution\n",
    "            mid_channels if expand_ratio != 1 else in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels) # batch normalization after the reduction convolution\n",
    "        \n",
    "        self.use_residual = in_channels == out_channels and stride == 1 # residual connection is apply if the input and output channel match and the stride is 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x # store x as identity for the residual connection\n",
    "        \n",
    "        if self.expand_ratio != 1: #apply 1x1 convolution , batch normalization and Silu activation \n",
    "            x = self.expand_conv(x)\n",
    "            x = self.bn0(x)\n",
    "            x = nn.functional.silu(x)\n",
    "        \n",
    "        x = self.depthwise_conv(x) # apply depthwise convolution , batch normalization andSilu activation \n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.silu(x)\n",
    "        \n",
    "        se = self.se(x) # compute attention weight through SE block and scales the channel\n",
    "        x = x * se\n",
    "        \n",
    "        x = self.reduce_conv(x) # apply 1x1 convolution and batch normalization to reduce channel\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        if self.use_residual:\n",
    "            x = x + identity\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00471d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, config, pretrained=False):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, config['stem_channels'], kernel_size=3, stride=2, padding=1, bias=False), # 3x3 convolution with stride 2 (downsampling spatial dimension by 2)\n",
    "            nn.BatchNorm2d(config['stem_channels']),# normalize the output\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_channels = config['stem_channels']\n",
    "        \n",
    "        for stage in config['stages']: # iterate over stage in config. where each stage define a group of blocks\n",
    "            num_layers = stage['num_layers']\n",
    "            out_channels = stage['out_channels']\n",
    "            expand_ratio = stage['expand_ratio']\n",
    "            kernel_size = stage['kernel_size']\n",
    "            stride = stage['stride']\n",
    "            \n",
    "            for i in range(num_layers):\n",
    "                block_stride = stride if i == 0 else 1\n",
    "                self.blocks.append(\n",
    "                    CustomMBConvBlock(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=block_stride,\n",
    "                        se_ratio=0.25\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "        \n",
    "        self.head = nn.Sequential( #final convolutional layer to prepare features for pooling \n",
    "            nn.Conv2d(in_channels, config['head_channels'], kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(config['head_channels']),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1) # average pooling to reduce spatial dimension to 1x1\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(config['head_channels'], 1000)\n",
    "        )\n",
    "        \n",
    "        if not pretrained:\n",
    "            self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d): #set weight to 1 and bias to 0\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x) \n",
    "        for block in self.blocks:\n",
    "            x = block(x) \n",
    "        x = self.head(x) \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a7f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# MODIFIED EFFICIENTNETFER CLASS\n",
    "# =======================\n",
    "\n",
    "class EfficientNetFER(nn.Module):\n",
    "    def __init__(self, num_classes=7, pretrained=False, device='cpu', custom_fc_dims=[512, 256], \n",
    "                 dropout=0.3, use_post_conv=False, efficientnet_config=None):\n",
    "        super(EfficientNetFER, self).__init__()\n",
    "        self.device = device\n",
    "        self.use_post_conv = use_post_conv\n",
    "        \n",
    "        default_config = {\n",
    "            'stem_channels': 32, # number of output channel for initial convolutional layer\n",
    "            'head_channels': 1280, # number of channel before the final classifier\n",
    "            'stages': [\n",
    "                {'num_layers': 1, 'out_channels': 16, 'expand_ratio': 1, 'kernel_size': 3, 'stride': 1},\n",
    "                {'num_layers': 2, 'out_channels': 24, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 2},\n",
    "                {'num_layers': 2, 'out_channels': 40, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 2},\n",
    "                {'num_layers': 3, 'out_channels': 80, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 2},\n",
    "                {'num_layers': 3, 'out_channels': 112, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 1},\n",
    "                {'num_layers': 4, 'out_channels': 192, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 2},\n",
    "                {'num_layers': 1, 'out_channels': 320, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 1},\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.config = efficientnet_config if efficientnet_config else default_config\n",
    "        \n",
    "        self.efficientnet = CustomEfficientNet(self.config, pretrained=pretrained)\n",
    "        self.efficientnet.to(device)\n",
    "        \n",
    "        for param in self.efficientnet.parameters():\n",
    "            param.requires_grad = False # disable gradient computation for all Efficientnet parameter\n",
    "        \n",
    "        if self.use_post_conv:\n",
    "            self.post_conv = nn.Sequential(# additional convolutional block is added after the efficientnet head\n",
    "                nn.Conv2d(self.config['head_channels'], 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512), \n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d(1) # reduce the spatial dimension to 1x1 (global average pooling)\n",
    "            ).to(device)\n",
    "            fc_input_features = 512\n",
    "        else:\n",
    "            fc_input_features = self.config['head_channels'] # 1280\n",
    "        \n",
    "        layers = [nn.Dropout(dropout)] # dropout to prevent overfitting\n",
    "        in_dim = fc_input_features\n",
    "        for out_dim in custom_fc_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            in_dim = out_dim \n",
    "        layers.append(nn.Linear(in_dim, num_classes)) # set a final linear to map the num class\n",
    "        \n",
    "        self.classifier = nn.Sequential(*layers).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.efficientnet.stem(x)\n",
    "        for block in self.efficientnet.blocks:\n",
    "            x = block(x)# iterate through each block , applying the efficientnet architecture\n",
    "        x = self.efficientnet.head(x)\n",
    "        \n",
    "        if self.use_post_conv:\n",
    "            x = self.post_conv(x)\n",
    "        else:\n",
    "            x = self.efficientnet.avgpool(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def unfreeze_layers(self, num_layers=5):# unfreeze specific layer for fine tuning , allow the parameter to be updated during training\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        layers = list(self.efficientnet.blocks[-num_layers:]) + [self.efficientnet.head] # select  the last numlayers blocks of the efficient and head\n",
    "        if self.use_post_conv:\n",
    "            layers.append(self.post_conv)\n",
    "        for layer in layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, device='cuda', patience=5):\n",
    "    #criterion (loss function)\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train() # put layer to training mode ( dropout/batch-norm)\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in tqdm(train_loader, desc='Training'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels) # compute the loss between prediction and true label \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset) \n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset) \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        with torch.inference_mode:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_corrects = 0\n",
    "            with torch.no_grad():# disable the gradient computation as gradient are not needed during validation\n",
    "                for inputs, labels in tqdm(val_loader, desc='Validation'):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1) # get predicted class indices\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    val_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            val_epoch_loss = val_loss / len(val_loader.dataset) \n",
    "            val_epoch_acc = val_corrects.double() / len(val_loader.dataset) \n",
    "            history['val_loss'].append(val_epoch_loss)\n",
    "            history['val_acc'].append(val_epoch_acc.item())\n",
    "            print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(val_epoch_loss) # use validation loss to decide whether to reduce learning rate \n",
    "        else:\n",
    "            scheduler.step() # update the learning rate based on the epoch count \n",
    "        \n",
    "        if val_epoch_acc > best_acc:\n",
    "            best_acc = val_epoch_acc\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        if epoch - best_epoch > patience:\n",
    "            print(f'Early stopping at epoch {epoch}') # if the validation accuracy hasnt improved , training stop to prevent overfitting \n",
    "            break\n",
    "    \n",
    "    print(f'Best val Acc: {best_acc:.4f} at epoch {best_epoch}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab82d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n",
      "ModuleList(\n",
      "  (0): CustomMBConvBlock(\n",
      "    (depthwise_conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "    (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "    (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "    (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (5): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "    (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (6-7): 2 x CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "    (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (8): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "    (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (9-10): 2 x CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "    (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (11): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "    (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (12-14): 3 x CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "    (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (15): CustomMBConvBlock(\n",
      "    (expand_conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn0): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise_conv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "    (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (se): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): SiLU()\n",
      "      (3): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "    (reduce_conv): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              SiLU-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      " AdaptiveAvgPool2d-6             [-1, 32, 1, 1]               0\n",
      "            Conv2d-7              [-1, 8, 1, 1]             264\n",
      "              SiLU-8              [-1, 8, 1, 1]               0\n",
      "            Conv2d-9             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-10             [-1, 32, 1, 1]               0\n",
      "           Conv2d-11         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-12         [-1, 16, 112, 112]              32\n",
      "CustomMBConvBlock-13         [-1, 16, 112, 112]               0\n",
      "           Conv2d-14         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-15         [-1, 96, 112, 112]             192\n",
      "           Conv2d-16           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-17           [-1, 96, 56, 56]             192\n",
      "AdaptiveAvgPool2d-18             [-1, 96, 1, 1]               0\n",
      "           Conv2d-19              [-1, 4, 1, 1]             388\n",
      "             SiLU-20              [-1, 4, 1, 1]               0\n",
      "           Conv2d-21             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-22             [-1, 96, 1, 1]               0\n",
      "           Conv2d-23           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-24           [-1, 24, 56, 56]              48\n",
      "CustomMBConvBlock-25           [-1, 24, 56, 56]               0\n",
      "           Conv2d-26          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-27          [-1, 144, 56, 56]             288\n",
      "           Conv2d-28          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-29          [-1, 144, 56, 56]             288\n",
      "AdaptiveAvgPool2d-30            [-1, 144, 1, 1]               0\n",
      "           Conv2d-31              [-1, 6, 1, 1]             870\n",
      "             SiLU-32              [-1, 6, 1, 1]               0\n",
      "           Conv2d-33            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-34            [-1, 144, 1, 1]               0\n",
      "           Conv2d-35           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-36           [-1, 24, 56, 56]              48\n",
      "CustomMBConvBlock-37           [-1, 24, 56, 56]               0\n",
      "           Conv2d-38          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-39          [-1, 144, 56, 56]             288\n",
      "           Conv2d-40          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-41          [-1, 144, 28, 28]             288\n",
      "AdaptiveAvgPool2d-42            [-1, 144, 1, 1]               0\n",
      "           Conv2d-43              [-1, 6, 1, 1]             870\n",
      "             SiLU-44              [-1, 6, 1, 1]               0\n",
      "           Conv2d-45            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-46            [-1, 144, 1, 1]               0\n",
      "           Conv2d-47           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-48           [-1, 40, 28, 28]              80\n",
      "CustomMBConvBlock-49           [-1, 40, 28, 28]               0\n",
      "           Conv2d-50          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-51          [-1, 240, 28, 28]             480\n",
      "           Conv2d-52          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-53          [-1, 240, 28, 28]             480\n",
      "AdaptiveAvgPool2d-54            [-1, 240, 1, 1]               0\n",
      "           Conv2d-55             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-56             [-1, 10, 1, 1]               0\n",
      "           Conv2d-57            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-58            [-1, 240, 1, 1]               0\n",
      "           Conv2d-59           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-60           [-1, 40, 28, 28]              80\n",
      "CustomMBConvBlock-61           [-1, 40, 28, 28]               0\n",
      "           Conv2d-62          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-63          [-1, 240, 28, 28]             480\n",
      "           Conv2d-64          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
      "AdaptiveAvgPool2d-66            [-1, 240, 1, 1]               0\n",
      "           Conv2d-67             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-68             [-1, 10, 1, 1]               0\n",
      "           Conv2d-69            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-70            [-1, 240, 1, 1]               0\n",
      "           Conv2d-71           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-72           [-1, 80, 14, 14]             160\n",
      "CustomMBConvBlock-73           [-1, 80, 14, 14]               0\n",
      "           Conv2d-74          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-75          [-1, 480, 14, 14]             960\n",
      "           Conv2d-76          [-1, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-77          [-1, 480, 14, 14]             960\n",
      "AdaptiveAvgPool2d-78            [-1, 480, 1, 1]               0\n",
      "           Conv2d-79             [-1, 20, 1, 1]           9,620\n",
      "             SiLU-80             [-1, 20, 1, 1]               0\n",
      "           Conv2d-81            [-1, 480, 1, 1]          10,080\n",
      "          Sigmoid-82            [-1, 480, 1, 1]               0\n",
      "           Conv2d-83           [-1, 80, 14, 14]          38,400\n",
      "      BatchNorm2d-84           [-1, 80, 14, 14]             160\n",
      "CustomMBConvBlock-85           [-1, 80, 14, 14]               0\n",
      "           Conv2d-86          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-87          [-1, 480, 14, 14]             960\n",
      "           Conv2d-88          [-1, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-89          [-1, 480, 14, 14]             960\n",
      "AdaptiveAvgPool2d-90            [-1, 480, 1, 1]               0\n",
      "           Conv2d-91             [-1, 20, 1, 1]           9,620\n",
      "             SiLU-92             [-1, 20, 1, 1]               0\n",
      "           Conv2d-93            [-1, 480, 1, 1]          10,080\n",
      "          Sigmoid-94            [-1, 480, 1, 1]               0\n",
      "           Conv2d-95           [-1, 80, 14, 14]          38,400\n",
      "      BatchNorm2d-96           [-1, 80, 14, 14]             160\n",
      "CustomMBConvBlock-97           [-1, 80, 14, 14]               0\n",
      "           Conv2d-98          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-99          [-1, 480, 14, 14]             960\n",
      "          Conv2d-100          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-101          [-1, 480, 14, 14]             960\n",
      "AdaptiveAvgPool2d-102            [-1, 480, 1, 1]               0\n",
      "          Conv2d-103             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-104             [-1, 20, 1, 1]               0\n",
      "          Conv2d-105            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-106            [-1, 480, 1, 1]               0\n",
      "          Conv2d-107          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-108          [-1, 112, 14, 14]             224\n",
      "CustomMBConvBlock-109          [-1, 112, 14, 14]               0\n",
      "          Conv2d-110          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-111          [-1, 672, 14, 14]           1,344\n",
      "          Conv2d-112          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-113          [-1, 672, 14, 14]           1,344\n",
      "AdaptiveAvgPool2d-114            [-1, 672, 1, 1]               0\n",
      "          Conv2d-115             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-116             [-1, 28, 1, 1]               0\n",
      "          Conv2d-117            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-118            [-1, 672, 1, 1]               0\n",
      "          Conv2d-119          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-120          [-1, 112, 14, 14]             224\n",
      "CustomMBConvBlock-121          [-1, 112, 14, 14]               0\n",
      "          Conv2d-122          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-123          [-1, 672, 14, 14]           1,344\n",
      "          Conv2d-124          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-125          [-1, 672, 14, 14]           1,344\n",
      "AdaptiveAvgPool2d-126            [-1, 672, 1, 1]               0\n",
      "          Conv2d-127             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-128             [-1, 28, 1, 1]               0\n",
      "          Conv2d-129            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-130            [-1, 672, 1, 1]               0\n",
      "          Conv2d-131          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-132          [-1, 112, 14, 14]             224\n",
      "CustomMBConvBlock-133          [-1, 112, 14, 14]               0\n",
      "          Conv2d-134          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-135          [-1, 672, 14, 14]           1,344\n",
      "          Conv2d-136            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-137            [-1, 672, 7, 7]           1,344\n",
      "AdaptiveAvgPool2d-138            [-1, 672, 1, 1]               0\n",
      "          Conv2d-139             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-140             [-1, 28, 1, 1]               0\n",
      "          Conv2d-141            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-142            [-1, 672, 1, 1]               0\n",
      "          Conv2d-143            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-144            [-1, 192, 7, 7]             384\n",
      "CustomMBConvBlock-145            [-1, 192, 7, 7]               0\n",
      "          Conv2d-146           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-147           [-1, 1152, 7, 7]           2,304\n",
      "          Conv2d-148           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-149           [-1, 1152, 7, 7]           2,304\n",
      "AdaptiveAvgPool2d-150           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-151             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-152             [-1, 48, 1, 1]               0\n",
      "          Conv2d-153           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-154           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-155            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-156            [-1, 192, 7, 7]             384\n",
      "CustomMBConvBlock-157            [-1, 192, 7, 7]               0\n",
      "          Conv2d-158           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-159           [-1, 1152, 7, 7]           2,304\n",
      "          Conv2d-160           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-161           [-1, 1152, 7, 7]           2,304\n",
      "AdaptiveAvgPool2d-162           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-163             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-164             [-1, 48, 1, 1]               0\n",
      "          Conv2d-165           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-166           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-167            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-168            [-1, 192, 7, 7]             384\n",
      "CustomMBConvBlock-169            [-1, 192, 7, 7]               0\n",
      "          Conv2d-170           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-171           [-1, 1152, 7, 7]           2,304\n",
      "          Conv2d-172           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-173           [-1, 1152, 7, 7]           2,304\n",
      "AdaptiveAvgPool2d-174           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-175             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-176             [-1, 48, 1, 1]               0\n",
      "          Conv2d-177           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-178           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-179            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-180            [-1, 192, 7, 7]             384\n",
      "CustomMBConvBlock-181            [-1, 192, 7, 7]               0\n",
      "          Conv2d-182           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-183           [-1, 1152, 7, 7]           2,304\n",
      "          Conv2d-184           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-185           [-1, 1152, 7, 7]           2,304\n",
      "AdaptiveAvgPool2d-186           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-187             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-188             [-1, 48, 1, 1]               0\n",
      "          Conv2d-189           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-190           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-191            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-192            [-1, 320, 7, 7]             640\n",
      "CustomMBConvBlock-193            [-1, 320, 7, 7]               0\n",
      "          Conv2d-194           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-195           [-1, 1280, 7, 7]           2,560\n",
      "            SiLU-196           [-1, 1280, 7, 7]               0\n",
      "          Conv2d-197            [-1, 512, 7, 7]       5,898,752\n",
      "     BatchNorm2d-198            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-199            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-200            [-1, 512, 1, 1]               0\n",
      "         Dropout-201                  [-1, 512]               0\n",
      "          Linear-202                  [-1, 512]         262,656\n",
      "            ReLU-203                  [-1, 512]               0\n",
      "         Dropout-204                  [-1, 512]               0\n",
      "          Linear-205                  [-1, 256]         131,328\n",
      "            ReLU-206                  [-1, 256]               0\n",
      "         Dropout-207                  [-1, 256]               0\n",
      "          Linear-208                    [-1, 7]           1,799\n",
      "================================================================\n",
      "Total params: 10,303,107\n",
      "Trainable params: 6,295,559\n",
      "Non-trainable params: 4,007,548\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 111.59\n",
      "Params size (MB): 39.30\n",
      "Estimated Total Size (MB): 151.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Model Setup\n",
    "\n",
    "batch_size = 128 \n",
    "num_epochs = 50 # number of time entire training dataset is passed through model\n",
    "learning_rate = 0.001\n",
    "num_classes = 7\n",
    "\n",
    "if torch.cuda.is_available():# determine to use GPU or CPU for training \n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "train_dir = os.path.join(base_dir, \"processed_data\", \"train\")\n",
    "val_dir = os.path.join(base_dir, \"processed_data\", \"validation\")\n",
    "\n",
    "transformation = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True), # Convert image to float ranging from 0 - 1\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_dir, transform=transformation)\n",
    "val_dataset = ImageFolder(val_dir, transform=transformation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "custom_config = {\n",
    "    'stem_channels': 32, # number of output channel in the initial layer of the network. Stem is the first convolutional layer that process the input image\n",
    "    'head_channels': 1280, #number of channels in the final feature map \n",
    "    'stages': [\n",
    "        {'num_layers': 1, 'out_channels': 16, 'expand_ratio': 1, 'kernel_size': 3, 'stride': 1},\n",
    "        {'num_layers': 2, 'out_channels': 24, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 2},\n",
    "        {'num_layers': 2, 'out_channels': 40, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 2},\n",
    "        {'num_layers': 3, 'out_channels': 80, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 2},\n",
    "        {'num_layers': 3, 'out_channels': 112, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 1},\n",
    "        {'num_layers': 4, 'out_channels': 192, 'expand_ratio': 6, 'kernel_size': 5, 'stride': 2},\n",
    "        {'num_layers': 1, 'out_channels': 320, 'expand_ratio': 6, 'kernel_size': 3, 'stride': 1},\n",
    "    ]\n",
    "}\n",
    "\n",
    "model = EfficientNetFER(\n",
    "    num_classes=num_classes,\n",
    "    device=device,\n",
    "    pretrained=False,\n",
    "    custom_fc_dims=[512, 256],\n",
    "    dropout=0.4,\n",
    "    use_post_conv=True,\n",
    "    efficientnet_config=custom_config\n",
    ")\n",
    "\n",
    "print(model.efficientnet.blocks)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()# combine log softmax and negative log likelihood loss , suitable for multi class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)#reduce learning rate by a factor of 0.1 if validation loss doesnt improve for 3 epochs. Help model converge better\n",
    "\n",
    "# Print out model summary\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77031586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/225 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Model Training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m model.unfreeze_layers(num_layers=\u001b[32m5\u001b[39m)\u001b[38;5;66;03m#unfreeze the last 5 layers to allow them to adapt the FER task\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, patience)\u001b[39m\n\u001b[32m     16\u001b[39m inputs, labels = inputs.to(device), labels.to(device)\n\u001b[32m     17\u001b[39m optimizer.zero_grad() \n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m _, preds = torch.max(outputs, \u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m loss = criterion(outputs, labels) \u001b[38;5;66;03m# compute the loss between prediction and true label \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mEfficientNetFER.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     59\u001b[39m x = \u001b[38;5;28mself\u001b[39m.efficientnet.stem(x)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.efficientnet.blocks:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# iterate through each block , applying the efficientnet architecture\u001b[39;00m\n\u001b[32m     62\u001b[39m x = \u001b[38;5;28mself\u001b[39m.efficientnet.head(x)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_post_conv:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mCustomMBConvBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.expand_ratio != \u001b[32m1\u001b[39m: \u001b[38;5;66;03m#apply 1x1 convolution , batch normalization and Silu activation \u001b[39;00m\n\u001b[32m     50\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.expand_conv(x)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     x = nn.functional.silu(x)\n\u001b[32m     54\u001b[39m x = \u001b[38;5;28mself\u001b[39m.depthwise_conv(x) \u001b[38;5;66;03m# apply depthwise convolution , batch normalization andSilu activation \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## Model Training\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                          num_epochs=num_epochs, device=device)\n",
    "    \n",
    "print(\"Starting fine-tuning...\")\n",
    "model.unfreeze_layers(num_layers=5)#unfreeze the last 5 layers to allow them to adapt the FER task\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate / 10)# create a new optimizer only for unfrozen parameter with reduced learning rate to make more smaller and more precise update\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "history_ft = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                            num_epochs=num_epochs // 2, device=device)\n",
    "\n",
    "for key in history:\n",
    "    history[key].extend(history_ft[key])# combine the training and fine tuning histories into single history dictionary .\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking and saving trained model\n",
    "\n",
    "# Print model's state dictionary\n",
    "for paramTensor in model.state_dict():\n",
    "    print(paramTensor, \"\\t\", model.state_dict()[paramTensor].size())\n",
    "\n",
    "# Print optimizer's state dictionary\n",
    "for varName in optimizer.state_dict():\n",
    "    print(varName, \"\\t\", optimizer.state_dict()[varName])\n",
    "\n",
    "# Save trained model\n",
    "\n",
    "base_path = os.getcwd()\n",
    "folder_path = os.path.join(base_path, \"model_files\")\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "model_name = \"model_1\"\n",
    "\n",
    "model_path = os.path.join(folder_path, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "modelStateDictName = model_name + \"_weights.pth\"\n",
    "modelEntireName = model_name + \".pth\"\n",
    "\n",
    "modelStateDict_path = os.path.join(model_path, modelStateDictName)\n",
    "modelEntire_path = os.path.join(model_path, modelEntireName)\n",
    "\n",
    "# Save model's weight only\n",
    "if os.path.isdir(modelStateDict_path):\n",
    "    print(f\"\\n{model_name} state dictionary file existed previously. New {model_name}'s state is not saved.\")\n",
    "else:\n",
    "    torch.save(model.state_dict(), modelStateDict_path)\n",
    "    print(f\"\\n{model_name}'s state file has been successfully saved.\")\n",
    "\n",
    "# Save entire model including layers and weights\n",
    "if os.path.isdir(modelEntire_path):\n",
    "    print(f\"\\n{model_name} file existed previously. New {model_name} is not saved.\")\n",
    "else:\n",
    "    torch.save(model, modelEntire_path)\n",
    "    print(f\"\\n{model_name} has been successfully saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393036b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing model's training history dictionary to file\n",
    "\n",
    "json_path = os.path.join(model_path, \"history.json\")\n",
    "\n",
    "if os.path.isdir(json_path):\n",
    "    print(f\"\\n{model_name}'s history file existed previously. New {model_name}'s history file is not saved.\")\n",
    "else:\n",
    "    with open(json_path, 'w') as file:\n",
    "        json.dump(history, file)\n",
    "    \n",
    "    print(f\"{model_name}'s training history has been saved.\")\n",
    "\n",
    "# Checking model's training history\n",
    "\n",
    "for x in history:\n",
    "    print(history)\n",
    "    for y in history[x]:\n",
    "        print(y, ':', history[x][y])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
